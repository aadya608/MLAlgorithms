{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Mean Square Error</H2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Mean square error (MSE) is a commonly used metric in statistical modeling and machine learning to measure how well a model fits the data. It is calculated as the average of the squared differences between the predicted values and the actual values. Specifically, for a set of n data points with predicted values ŷi and actual values yi, the MSE is:\n",
    "\n",
    "MSE = 1/n * Σ(i=1 to n) (yi - ŷi)^2\n",
    "\n",
    "It is also called the <b>Cost Function</b>\n",
    "\n",
    "<b>The MSE value provides a quantitative measure of how much the predicted values deviate from the actual values. A smaller MSE indicates that the model fits the data better.\n",
    "\n",
    "</b>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Gradient Descent</H2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Gradient descent is a popular optimization algorithm used in machine learning to find the parameters of a model that minimize the objective function. The algorithm works by iteratively updating the model parameters in the direction of the steepest descent of the objective function.\n",
    "\n",
    "To implement gradient descent, we first initialize the model parameters to some initial values. We then compute the gradient of the objective function with respect to the model parameters. The gradient indicates the direction in which the objective function is increasing the fastest, and so we want to move in the opposite direction to minimize the objective function. We update the model parameters by taking a small step in the negative direction of the gradient. This process is repeated until convergence, where the objective function stops decreasing.\n",
    "\n",
    "The size of the step taken in each iteration is controlled by a hyperparameter called the learning rate. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too large, the algorithm may overshoot the minimum of the objective function and fail to converge. Therefore, choosing an appropriate learning rate is crucial for the success of the gradient descent algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51f2cd60ca62dd32de3ad1be2c33cede715dc3ac8221a1cb70abcd44eb0ee1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
